{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqq4Cte5Cz9sC7aCOCL5Zd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvisanth/Association/blob/master/AGI_sample_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNgB9WCNW43E",
        "outputId": "88dd9cce-1300-4bc1-c9b6-c3157bee478f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 Total Reward: 213.0\n",
            "Episode: 1 Total Reward: 82.0\n",
            "Episode: 2 Total Reward: 1\n",
            "Episode: 3 Total Reward: 531.0\n",
            "Episode: 4 Total Reward: 116.0\n",
            "Episode: 5 Total Reward: 42.5\n",
            "Episode: 6 Total Reward: 74.5\n",
            "Episode: 7 Total Reward: 22.5\n",
            "Episode: 8 Total Reward: 75.0\n",
            "Episode: 9 Total Reward: 1\n",
            "Episode: 10 Total Reward: 380.0\n",
            "Episode: 11 Total Reward: 104.5\n",
            "Episode: 12 Total Reward: 5.5\n",
            "Episode: 13 Total Reward: 438.0\n",
            "Episode: 14 Total Reward: 261.5\n",
            "Episode: 15 Total Reward: 153.5\n",
            "Episode: 16 Total Reward: 151.0\n",
            "Episode: 17 Total Reward: 621.0\n",
            "Episode: 18 Total Reward: 338.0\n",
            "Episode: 19 Total Reward: 5.0\n",
            "Episode: 20 Total Reward: 82.5\n",
            "Episode: 21 Total Reward: 179.0\n",
            "Episode: 22 Total Reward: 73.5\n",
            "Episode: 23 Total Reward: 871.5\n",
            "Episode: 24 Total Reward: 161.5\n",
            "Episode: 25 Total Reward: 442.5\n",
            "Episode: 26 Total Reward: 214.5\n",
            "Episode: 27 Total Reward: 521.5\n",
            "Episode: 28 Total Reward: 26.0\n",
            "Episode: 29 Total Reward: 136.0\n",
            "Episode: 30 Total Reward: 56.5\n",
            "Episode: 31 Total Reward: 70.0\n",
            "Episode: 32 Total Reward: 1231.0\n",
            "Episode: 33 Total Reward: 1\n",
            "Episode: 34 Total Reward: 82.0\n",
            "Episode: 35 Total Reward: 24.5\n",
            "Episode: 36 Total Reward: 323.0\n",
            "Episode: 37 Total Reward: 58.0\n",
            "Episode: 38 Total Reward: 170.0\n",
            "Episode: 39 Total Reward: 140.0\n",
            "Episode: 40 Total Reward: 16.0\n",
            "Episode: 41 Total Reward: 1786.5\n",
            "Episode: 42 Total Reward: 592.5\n",
            "Episode: 43 Total Reward: 301.5\n",
            "Episode: 44 Total Reward: 53.0\n",
            "Episode: 45 Total Reward: 1197.0\n",
            "Episode: 46 Total Reward: 24.0\n",
            "Episode: 47 Total Reward: 151.5\n",
            "Episode: 48 Total Reward: 372.0\n",
            "Episode: 49 Total Reward: 3.0\n",
            "Episode: 50 Total Reward: 77.5\n",
            "Episode: 51 Total Reward: 272.5\n",
            "Episode: 52 Total Reward: 99.5\n",
            "Episode: 53 Total Reward: 394.5\n",
            "Episode: 54 Total Reward: 1462.5\n",
            "Episode: 55 Total Reward: 470.5\n",
            "Episode: 56 Total Reward: 30.5\n",
            "Episode: 57 Total Reward: 144.0\n",
            "Episode: 58 Total Reward: 58.5\n",
            "Episode: 59 Total Reward: 272.0\n",
            "Episode: 60 Total Reward: 368.0\n",
            "Episode: 61 Total Reward: 130.0\n",
            "Episode: 62 Total Reward: 213.0\n",
            "Episode: 63 Total Reward: 145.5\n",
            "Episode: 64 Total Reward: 25.5\n",
            "Episode: 65 Total Reward: 1\n",
            "Episode: 66 Total Reward: 464.5\n",
            "Episode: 67 Total Reward: 105.5\n",
            "Episode: 68 Total Reward: 27.5\n",
            "Episode: 69 Total Reward: 119.5\n",
            "Episode: 70 Total Reward: 1\n",
            "Episode: 71 Total Reward: 280.0\n",
            "Episode: 72 Total Reward: 400.0\n",
            "Episode: 73 Total Reward: 233.0\n",
            "Episode: 74 Total Reward: 224.5\n",
            "Episode: 75 Total Reward: 35.0\n",
            "Episode: 76 Total Reward: 581.0\n",
            "Episode: 77 Total Reward: 512.0\n",
            "Episode: 78 Total Reward: 721.5\n",
            "Episode: 79 Total Reward: 168.5\n",
            "Episode: 80 Total Reward: 177.5\n",
            "Episode: 81 Total Reward: 749.5\n",
            "Episode: 82 Total Reward: 413.5\n",
            "Episode: 83 Total Reward: 130.5\n",
            "Episode: 84 Total Reward: 60.0\n",
            "Episode: 85 Total Reward: 859.5\n",
            "Episode: 86 Total Reward: 162.5\n",
            "Episode: 87 Total Reward: 43.5\n",
            "Episode: 88 Total Reward: 62.0\n",
            "Episode: 89 Total Reward: 58.0\n",
            "Episode: 90 Total Reward: 117.5\n",
            "Episode: 91 Total Reward: 64.5\n",
            "Episode: 92 Total Reward: 243.5\n",
            "Episode: 93 Total Reward: 272.5\n",
            "Episode: 94 Total Reward: 360.5\n",
            "Episode: 95 Total Reward: 128.5\n",
            "Episode: 96 Total Reward: 362.5\n",
            "Episode: 97 Total Reward: 30.0\n",
            "Episode: 98 Total Reward: 20.5\n",
            "Episode: 99 Total Reward: 108.0\n",
            "Average Total Reward across Evaluation Episodes: 16.035\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Define the environment class\n",
        "class Environment:\n",
        "    def __init__(self, grid_size, agent_position, goal_position, obstacle_positions, reward_positions):\n",
        "        self.grid_size = grid_size\n",
        "        self.agent_position = agent_position\n",
        "        self.goal_position = goal_position\n",
        "        self.obstacle_positions = obstacle_positions\n",
        "        self.reward_positions = reward_positions\n",
        "\n",
        "    def get_current_state(self):\n",
        "        return self.agent_position\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        row, col = state\n",
        "        possible_actions = []\n",
        "        if row > 0:  # Up\n",
        "            possible_actions.append('up')\n",
        "        if row < self.grid_size - 1:  # Down\n",
        "            possible_actions.append('down')\n",
        "        if col > 0:  # Left\n",
        "            possible_actions.append('left')\n",
        "        if col < self.grid_size - 1:  # Right\n",
        "            possible_actions.append('right')\n",
        "        return possible_actions\n",
        "\n",
        "    def execute_action(self, action):\n",
        "        row, col = self.agent_position\n",
        "        if action == 'up' and row > 0 and (row - 1, col) not in self.obstacle_positions:\n",
        "            self.agent_position = (row - 1, col)\n",
        "        elif action == 'down' and row < self.grid_size - 1 and (row + 1, col) not in self.obstacle_positions:\n",
        "            self.agent_position = (row + 1, col)\n",
        "        elif action == 'left' and col > 0 and (row, col - 1) not in self.obstacle_positions:\n",
        "            self.agent_position = (row, col - 1)\n",
        "        elif action == 'right' and col < self.grid_size - 1 and (row, col + 1) not in self.obstacle_positions:\n",
        "            self.agent_position = (row, col + 1)\n",
        "        return self.agent_position\n",
        "\n",
        "    def get_reward(self, position):\n",
        "        if position == self.goal_position:\n",
        "            return 1  # Positive reward for reaching the goal\n",
        "        elif position in self.reward_positions:\n",
        "            return 0.5  # Partial reward for collecting a reward\n",
        "        else:\n",
        "            return 0  # No reward for other positions\n",
        "\n",
        "# Define the agent class\n",
        "class Agent:\n",
        "    def __init__(self, environment, start_position, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
        "        self.environment = environment\n",
        "        self.position = start_position\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.q_table = {}  # Dictionary to store Q-values\n",
        "\n",
        "    def move(self, action):\n",
        "        # Execute action in the environment and update agent's position\n",
        "        next_position = self.environment.execute_action(action)\n",
        "        self.position = next_position\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        # Update Q-value using Q-learning update rule\n",
        "        q_value = self.q_table.get((state, action), 0.0)\n",
        "        max_next_q_value = max([self.q_table.get((next_state, a), 0.0) for a in self.environment.get_possible_actions(next_state)])\n",
        "        new_q_value = q_value + self.learning_rate * (reward + self.discount_factor * max_next_q_value - q_value)\n",
        "        self.q_table[(state, action)] = new_q_value\n",
        "\n",
        "    def decide_action(self, state):\n",
        "        if random.random() < self.exploration_rate:\n",
        "            # Explore: randomly select an action\n",
        "            return random.choice(self.environment.get_possible_actions(state))\n",
        "        else:\n",
        "            # Exploit: select action with the highest Q-value\n",
        "            return max(self.environment.get_possible_actions(state), key=lambda a: self.q_table.get((state, a), 0.0))\n",
        "\n",
        "# Define the grid-world environment\n",
        "grid_size = 5\n",
        "environment = [[0 for _ in range(grid_size)] for _ in range(grid_size)]\n",
        "agent_symbol = 'A'\n",
        "goal_symbol = 'G'\n",
        "obstacle_symbol = 'X'\n",
        "reward_symbol = 'R'\n",
        "empty_symbol = ' '\n",
        "agent_position = (0, 0)\n",
        "goal_position = (4, 4)\n",
        "obstacle_positions = [(1, 1), (2, 2)]\n",
        "reward_positions = [(3, 3)]\n",
        "environment[agent_position[0]][agent_position[1]] = agent_symbol\n",
        "environment[goal_position[0]][goal_position[1]] = goal_symbol\n",
        "for obstacle_pos in obstacle_positions:\n",
        "    environment[obstacle_pos[0]][obstacle_pos[1]] = obstacle_symbol\n",
        "for reward_pos in reward_positions:\n",
        "    environment[reward_pos[0]][reward_pos[1]] = reward_symbol\n",
        "\n",
        "# Define the number of episodes for training\n",
        "num_episodes = 100\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    # Reset environment for new episode\n",
        "    environment = Environment(grid_size, agent_position, goal_position, obstacle_positions, reward_positions)\n",
        "    # Initialize cumulative reward for the episode\n",
        "    total_reward = 0\n",
        "\n",
        "    # Initialize agent with the environment and starting position\n",
        "    agent = Agent(environment, agent_position)\n",
        "\n",
        "    # Loop within each episode until terminal condition is met\n",
        "    while True:\n",
        "        # Get current state\n",
        "        current_state = environment.get_current_state()\n",
        "\n",
        "        # Agent decides on action\n",
        "        action = agent.decide_action(current_state)\n",
        "\n",
        "        # Agent executes action in the environment\n",
        "        agent.move(action)\n",
        "\n",
        "        # Agent receives reward from the environment\n",
        "        reward = environment.get_reward(agent.position)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Get next state after action execution\n",
        "        next_state = environment.get_current_state()\n",
        "\n",
        "        # Update Q-values based on observed reward and next state\n",
        "        agent.update_q_value(current_state, action, reward, next_state)\n",
        "\n",
        "        # Check if terminal condition is met\n",
        "        if agent.position == environment.goal_position:\n",
        "            break\n",
        "\n",
        "    # Print total reward for the episode\n",
        "    print(\"Episode:\", episode, \"Total Reward:\", total_reward)\n",
        "\n",
        "# Define the number of evaluation episodes\n",
        "num_evaluation_episodes = 100\n",
        "\n",
        "# Evaluation loop\n",
        "total_rewards = []\n",
        "for episode in range(num_evaluation_episodes):\n",
        "    # Reset environment for new episode\n",
        "    environment = Environment(grid_size, agent_position, goal_position, obstacle_positions, reward_positions)\n",
        "    total_reward = 0\n",
        "\n",
        "    # Initialize agent with the environment and starting position\n",
        "    agent = Agent(environment, agent_position)\n",
        "\n",
        "    # Loop within each episode until terminal condition is met\n",
        "    while True:\n",
        "        # Get current state\n",
        "        current_state = environment.get_current_state()\n",
        "\n",
        "        # Agent selects action based on learned policy\n",
        "        action = agent.decide_action(current_state)\n",
        "\n",
        "        # Agent executes action in the environment\n",
        "        agent.move(action)\n",
        "\n",
        "        # Agent receives reward from the environment\n",
        "        reward = environment.get_reward(agent.position)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Check if terminal condition is met\n",
        "        if agent.position == environment.goal_position:\n",
        "            break\n",
        "\n",
        "    # Store total reward for the episode\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "# Print average total reward across evaluation episodes\n",
        "average_reward = sum(total_rewards) / num_evaluation_episodes\n",
        "print(\"Average Total Reward across Evaluation Episodes:\", average_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgRulB2RjK2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3qHQtvm7jL3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "58ZQh5q1jMO-"
      }
    }
  ]
}